import logging
import asyncio
from typing import List


from app.db.repositories.StockNewsRepository import NewsRepository
from app.schemas.stockNews import StockNews

logger = logging.getLogger("NewsService")

class NewsService:
    def __init__(self, crawler_factory, analyzer, news_repo: NewsRepository, concurrency_limit: int = 5):
        self.crawler_factory = crawler_factory
        self.analyzer = analyzer
        self.news_repo = news_repo  # Repository ì£¼ìž…
        self.semaphore = asyncio.Semaphore(concurrency_limit)

    async def process_news_list(self, items: List[StockNews]) -> List[StockNews]:
        if not items:
            return []

        # 1. í¬ë¡¤ë§ (ë³‘ë ¬ ì²˜ë¦¬)
        crawl_tasks = [self._fetch_content_safe(item) for item in items if not item.content]
        if crawl_tasks:
            await asyncio.gather(*crawl_tasks)

        # 2. ìœ íš¨ì„± ê²€ì‚¬
        valid_items = [item for item in items if self._is_valid(item.content)]


        if not valid_items:
            logger.info(f"âš ï¸ ì²˜ë¦¬í•  ìœ íš¨í•œ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. (ìš”ì²­: {len(items)}ê±´)")
            return []


        # 3. AI ë¶„ì„
        logger.info(f"ðŸ§  AI ë¶„ì„ ì‹œìž‘: {len(valid_items)}ê±´")
        try:
            analysis_results = self.analyzer.analyze_batch(valid_items)

      
            for item, analysis in zip(valid_items, analysis_results):
                item.sentiment = analysis.get('sentiment', 'neutral')
                item.impact_score = analysis.get('importance', 0)
                item.ai_summary = analysis.get('summary', '')

        except Exception as e:
            logger.error(f"âŒ AI ë¶„ì„ ë‹¨ê³„ ì—ëŸ¬: {e}")
            return []

        # 4. DB ì €ìž¥ (Repository ì‚¬ìš©)
        try:
            # ServiceëŠ” DynamoDB JSON ë³€í™˜ì„ ëª°ë¼ë„ ë¨. ê°ì²´ ê·¸ëŒ€ë¡œ ì „ë‹¬.
            await self.news_repo.save_news_batch(valid_items)
            logger.info(f"ðŸ’¾ DB ì €ìž¥ ì™„ë£Œ: {len(valid_items)}ê±´")
        except Exception as e:
            logger.error(f"âŒ ì €ìž¥ ì‹¤íŒ¨: {e}")

        return valid_items



    async def _fetch_content_safe(self, item: StockNews):
        async with self.semaphore:
            try:
                crawler = self.crawler_factory.get_crawler(item.source)
                content = await crawler.fetch(item.url)
                item.content = content

            except Exception as e:
                logger.warning(f"âš ï¸ í¬ë¡¤ë§ ì‹¤íŒ¨ ({item.source}): {item.url} -> {e}")
                item.content = None

    def _is_valid(self, content: str) -> bool:
        return bool(content and len(content.strip()) >= 50)

