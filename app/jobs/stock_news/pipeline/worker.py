import time
import asyncio
import logging
from app.db.StockNews import StockNews
from app.services.aws_service import put_item_dynamodb, put_items_batch_dynamodb

logger = logging.getLogger("NewsWorker")


class NewsWorker:
    def __init__(self, crawler_factory, analyzer, queue):
        """
        Dependency Injection (ì˜ì¡´ì„± ì£¼ì…)
        ì›Œì»¤ëŠ” êµ¬ì²´ì ì¸ êµ¬í˜„ ë‚´ìš©ì„ ëª°ë¼ë„ ë¨. 
        """
        self.crawler_factory = crawler_factory
        self.analyzer = analyzer
        self.queue = queue
        self.table_name = "StockProjectData"

    async def run(self, worker_id: int):
        logger.info(f"ğŸ‘· ì›Œì»¤ {worker_id}ë²ˆ ì¶œê·¼ ì™„ë£Œ")

        while True:
            # 1. íì—ì„œ ë°ì´í„° êº¼ë‚´ê¸° (ì—†ìœ¼ë©´ ëŒ€ê¸°)

            item: StockNews = await self.queue.get()

            try:
                await self._process_item(item)
            except Exception as e:
                logger.error(f"âŒ ì›Œì»¤ {worker_id} ì—ëŸ¬ ({item.id}): {e}")
            finally:
                self.queue.task_done()

    def _is_valid_for_analysis(self, content: str) -> bool:
        """AI ë¶„ì„ì„ ìˆ˜í–‰í•˜ê¸°ì— ì¶©ë¶„í•œ ë°ì´í„°ì¸ì§€ íŒë‹¨"""
        if not content:
            return False
        if len(content.strip()) < 50:  # ê³µë°± ì œê±° í›„ ê¸¸ì´ ì²´í¬ ë“± ë””í…Œì¼ ì¶”ê°€ ê°€ëŠ¥
            return False
        return True


    async def _process_item(self, item: StockNews):
        # A. í¬ë¡¤ë§ (Crawling)
        # íŒ©í† ë¦¬ì—ì„œ ì†ŒìŠ¤ì— ë§ëŠ” í¬ë¡¤ëŸ¬ë¥¼ ê°€ì ¸ì˜´
        crawler = self.crawler_factory.get_crawler(item.source)

        # contentê°€ ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ, ì—†ìœ¼ë©´ í¬ë¡¤ë§
        if not item.content:
            # fetch ë©”ì†Œë“œëŠ” urlë§Œ ë°›ë„ë¡ ë˜ì–´ ìˆìœ¼ë¯€ë¡œ í˜¸ì¶œ
            item.content = await crawler.fetch(item.url)

        # ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ë¶„ì„ ìŠ¤í‚µ
        if not self._is_valid_for_analysis(item.content):
            logger.warning(f"âš ï¸ ë¶„ì„ ë¶€ì í•©(ë‚´ìš© ë¶€ì¡±): {item.url}")
            return

        # B. AI ë¶„ì„ (Analysis)
        # QuickNewsAnalyzer.analyze ë©”ì†Œë“œ í˜¸ì¶œ
        # ë‹¨ê±´ ë¶„ì„ ë°©ì‹ìœ¼ë¡œ ì§„í–‰
        analysis_result = self.analyzer.analyze(item.content, item.symbol)

        # ê²°ê³¼ ë§¤í•‘
        item.sentiment = analysis_result.sentiment
        item.impact_score = analysis_result.importance
        item.ai_summary = analysis_result.summary

        # C. ì €ì¥ (Save)
        await put_item_dynamodb(table_name=self.table_name ,item = item.to_dynamodb_item())


class NewsBatchWorker:
    # ë°°ì¹˜ì— ìŒ“ì¸ ì¼ë“¤ì„ ì²˜ë¦¬í•˜ëŠ” ì›Œì»¤
    def __init__(self, crawler_factory, analyzer, queue, batch_size=10, batch_timeout=5.0):
        self.crawler_factory = crawler_factory
        self.analyzer = analyzer
        self.queue = queue
        self.table_name = "StockProjectData"
        self.batch_size = batch_size
        self.batch_timeout = batch_timeout
        self.semaphore = asyncio.Semaphore(3)

    def _is_valid_for_analysis(self, content: str) -> bool:
        """AI ë¶„ì„ì„ ìˆ˜í–‰í•˜ê¸°ì— ì¶©ë¶„í•œ ë°ì´í„°ì¸ì§€ íŒë‹¨"""
        if not content:
            return False
        if len(content.strip()) < 50:  # ê³µë°± ì œê±° í›„ ê¸¸ì´ ì²´í¬ ë“± ë””í…Œì¼ ì¶”ê°€ ê°€ëŠ¥
            return False
        return True

    async def run(self, worker_id: int = 0):
        logger.info(f"ğŸšœ ë°°ì¹˜ ì›Œì»¤ {worker_id}ë²ˆ ê°€ë™ ì‹œì‘")
        logger.info("NewsBatchWorker Started")
        buffer = []
        last_flush_time = time.time()  # ë§ˆì§€ë§‰ ì²˜ë¦¬ ì‹œê°„ ê¸°ë¡

        while True:
            try:
                # íì—ì„œ ê°€ì ¸ì˜¤ê¸° (íƒ€ì„ì•„ì›ƒ ì„¤ì •)
                # 1ì´ˆ ë™ì•ˆ ê¸°ë‹¤ë ¤ë³´ê³  ì—†ìœ¼ë©´ asyncio.TimeoutError ë°œìƒ
                item = await asyncio.wait_for(self.queue.get(), timeout=1.0)
                buffer.append(item)

            except asyncio.TimeoutError:
                pass  # íƒ€ì„ì•„ì›ƒ ë‚˜ë©´ í˜„ì¬ ë²„í¼ë¼ë„ ì²˜ë¦¬í•˜ëŸ¬ ê°

            # ì‹œê°„ ì²´í¬
            current_time = time.time()
            time_since_flush = current_time - last_flush_time

            # ë²„í¼ê°€ ê½‰ ì°¼ê±°ë‚˜, (ë°ì´í„°ê°€ ìˆëŠ”ë°) ì‹œê°„ì´ ì§€ë‚¬ë‹¤ë©´ ì²˜ë¦¬
            if len(buffer) >= self.batch_size or (buffer and time_since_flush >= self.batch_timeout):

                processed_count = len(buffer)

                try:
                    await self._process_batch(buffer)
                except Exception as e:
                    import traceback
                    logger.error(f"Batch Critical Error: {e}")
                    logger.error(traceback.format_exc())  # ì–´ë””ì„œ ì—ëŸ¬ ë‚¬ëŠ”ì§€ ì¤„ë²ˆí˜¸ í™•ì¸
                finally:
                    for _ in range(processed_count):
                        self.queue.task_done()


                buffer = []  # ë²„í¼ ì´ˆê¸°í™”
                last_flush_time = current_time

    async def _fetch_and_assign(self, item: StockNews):
        # ê°œë³„ ì•„ì´í…œì— ëŒ€í•´ í¬ë¡¤ë§ ìˆ˜í–‰-->ì´ë¥¼ _process_batchì—ì„œ ë³‘ë ¹ì ìœ¼ë¡œ ìˆ˜í–‰
        async with self.semaphore:
            try:
                crawler = self.crawler_factory.get_crawler(item.source)
                # ê°€ì ¸ì™€ì„œ ë°”ë¡œ ìê¸° ìì‹ (item)ì— ì§‘ì–´ë„£ìŒ
                content = await crawler.fetch(item.url)
                item.content = content
            except Exception as e:
                # ì—ëŸ¬ê°€ ë‚˜ë„ ì—¬ê¸°ì„œ ì²˜ë¦¬í•˜ê³ , itemì—ëŠ” Noneì´ë‚˜ ë¹ˆ ê°’ì„ ë‘ 
                logger.warning(f"Crawling failed for {item.url}: {e}")
                item.content = None


    async def _process_batch(self, items: list[StockNews]):

        # í¬ë¡¤ë§
        tasks = [
            self._fetch_and_assign(item)
            for item in items
            if not item.content
        ]

        if tasks:
            # ë‹¤ ëë‚  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼ (ê²°ê³¼ë¥¼ ë¦¬í„´ë°›ì„ í•„ìš” ì—†ìŒ! ì´ë¯¸ itemì´ ìˆ˜ì •ë¨)
            await asyncio.gather(*tasks)


        # ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ìœ¼ë¡œ ìœ íš¨í•œ ê²ƒë§Œ ì¬í• ë‹¹
        items[:] = [item for item in items if self._is_valid_for_analysis(item.content)]

        if not items:
            logger.info("âš ï¸ ë°°ì¹˜ ë‚´ ìœ íš¨í•œ ë‰´ìŠ¤ê°€ ì—†ì–´ ìŠ¤í‚µí•©ë‹ˆë‹¤.")
            return

        #ai ë¶„ì„
        analysis_results = self.analyzer.analyze_batch(
            items
        )

        #ë§¤í•‘
        for item, analysis in zip(items, analysis_results):
            item.sentiment = analysis['sentiment']
            item.impact_score = analysis['importance']
            item.ai_summary = analysis['summary']


        #ì €ì¥
        await put_items_batch_dynamodb(table_name=self.table_name, items=[item.to_dynamodb_item() for item in items])